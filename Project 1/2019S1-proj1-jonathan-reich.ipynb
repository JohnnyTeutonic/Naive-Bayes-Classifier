{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2019 Semester 1\n",
    "-----\n",
    "## Project 1: Gaining Information about Naive Bayes\n",
    "-----\n",
    "###### Student Name(s): Jonathan Reich\n",
    "###### Python version: 3.5.6,\n",
    "###### Submission deadline: 5 pm, Mon 8 Apr 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you may use for your Project 1 submission. (You are not required to use it; in particular, there is no need to use iPython if you do not like it.)\n",
    "\n",
    "Marking will be applied on the five functions that are defined in this notebook, and to your responses to the questions at the end of this notebook.\n",
    "\n",
    "You may change the prototypes of these functions, and you may write other functions, according to your requirements. We would appreciate it if the required functions were prominent/easy to find. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import copy\n",
    "import operator\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function should open a data file in csv, and transform it into a usable format \n",
    "def preprocess(file, split=0.0):\n",
    "    \"\"\"Opens a csv file of data and optionally transforms the data into 'training' and 'test' splits for\n",
    "    'holdout' strategy. Also generates a dictionary structure that\n",
    "    summarises the statistics about the number of instances of individual classes, as well as the number of instances \n",
    "    found in the complete dataset, and the number of attributes found in the dataset.\n",
    "    \n",
    "    Args:\n",
    "        file (csv file): dataset\n",
    "        split (number): optional number that splits the data into partitions; default is set to 0 for training on all data.\n",
    "    \n",
    "    Returns:\n",
    "        class_dict (dic): the number of instances found in the complete dataset.\n",
    "        cleaned_data_train (list): two-dimensional list of lists containing the training data.\n",
    "        cleaned_data_train (list): two-dimensional list of lists containing the test data.\n",
    "        n_instances_test (number) : the number of instances found in the test dataset.\n",
    "        class_dict (dic): dictionary giving counts of individual classes for the training dataset.\n",
    "        test_dict (dic): dictionary giving counts of individual classes for the test dataset\n",
    "        n_instances_tr (number): integer of the number of instances found in the training dataset.\n",
    "        n_attrs (number): the number of attributes/properties found in a given instance.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_attrs = 0\n",
    "    f = open(file, 'r')\n",
    "    # remember start of dataset for counting purposes.\n",
    "    x = f.tell()\n",
    "    first_line = f.readline().split(',')\n",
    "    # read off number of attributes.\n",
    "    for line in first_line[:-1]:\n",
    "        n_attrs+=1\n",
    "    # seek back to first line of dataset for counting purposes.\n",
    "    f.seek(x)\n",
    "    g = list(f)\n",
    "    f.close()\n",
    "    # count number of instances in whole dataset.\n",
    "    n_instances = len(g)\n",
    "    # if we want to use the 'holdout' strategy:\n",
    "    if split > 0:\n",
    "        cleaned_data_train = []\n",
    "        cleaned_data_test = []\n",
    "        test_dict=defaultdict(int)\n",
    "        train_dict=defaultdict(int)\n",
    "        # randomise the data before partitioning.\n",
    "        random.shuffle(g)\n",
    "        n_instances_tr = int(n_instances*split)\n",
    "        # training instances split.\n",
    "        training_data = g[:n_instances_tr]\n",
    "        # test instance split.\n",
    "        test_data = g[n_instances_tr:]\n",
    "        n_instances_test = len(test_data)\n",
    "        for line in training_data:\n",
    "            # count classes in training set.\n",
    "            train_dict[line.strip().split(\",\")[-1]] += 1\n",
    "            # list of cleaned training data.\n",
    "            cleaned_data_train.append(line.strip().split(','))\n",
    "        for line in test_data:\n",
    "            # count classes in test set.\n",
    "            cleaned_data_test.append(line.strip().split(','))\n",
    "            # list of cleaning test data.\n",
    "            test_dict[line.strip().split(\",\")[-1]] += 1\n",
    "        return cleaned_data_train, cleaned_data_test, n_instances_test, train_dict, test_dict, n_instances_tr, n_attrs\n",
    "    \n",
    "    cleaned_data = []\n",
    "    \n",
    "    class_dict=defaultdict(int)\n",
    "    \n",
    "    # No partitiong of data, so we will train on the full dataset:\n",
    "    for line in g:\n",
    "        # count classes in full dataset.\n",
    "        class_dict[line.strip().split(\",\")[-1]] += 1\n",
    "        # append the cleaned data to a list for further processing.\n",
    "        cleaned_data.append(line.strip().split(','))\n",
    "   \n",
    "    return class_dict, n_instances, cleaned_data, n_attrs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function should build a supervised NB model\n",
    "def train(training_data, n_instances, class_dict, n_attrs, ig_dic=False):\n",
    "    \"\"\"Trains the data from a preprocessed list and returns dictionaries of probabilities used to predict instances.\n",
    "    \n",
    "    Args:\n",
    "        training_data (list): preprocessed dataset.\n",
    "        n_instances (number): number of instances found in dataset.\n",
    "        class_dict (dic): counts of classes.\n",
    "        n_attrs (number): number of attributes found in a given instance.\n",
    "        ig_dic (dictionary): gives a summary of counts of individual attributes (for use for information gain metric).\n",
    "    \n",
    "    Returns:\n",
    "        prior (dic): dictionary of prior probabilities of classes.\n",
    "        post (dic): dictionary of posterior probabilities of classes.\n",
    "        training_data (list): training data in preprocessed form for next stage of the pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    prior = defaultdict(int)\n",
    "    # store the prior probabilities a dictionary.\n",
    "    for key,val in class_dict.items():\n",
    "        prior[key] = val/n_instances\n",
    "    \n",
    "    post=defaultdict(int)\n",
    "    \n",
    "    # iterate through the number of attributes.\n",
    "    for attr in range(n_attrs):\n",
    "        # create a nested dictionary for each attribute.\n",
    "        temp_dict = defaultdict(int)\n",
    "        for line in training_data:\n",
    "            cls = line[-1]\n",
    "            # create a key for each class in the nested dictionary.\n",
    "            if cls not in temp_dict:\n",
    "                temp_dict[cls] = defaultdict(int)\n",
    "            \n",
    "            # if the attribute itself doesn't already exist in the dictionary, count it as one.\n",
    "            if line[attr] not in temp_dict[cls]:\n",
    "                temp_dict[cls][line[attr]] = 1\n",
    "            # otherwise, increment the count (this class, attribute pair already exists.)\n",
    "            else:\n",
    "                temp_dict[cls][line[attr]] += 1\n",
    "        \n",
    "        # set the posteior attribute to the counter from the 'temp_dict' object.\n",
    "        post[attr] = temp_dict\n",
    "        \n",
    "        \n",
    "        # iterate through each class in the dataset.\n",
    "        for cls in prior.keys():\n",
    "            # find aggregate of the values for each class.\n",
    "            sum_value = sum(temp_dict[cls].values())\n",
    "            for key, val in temp_dict[cls].items():\n",
    "                # if parameter 'ig_dic' is true, then we will use the 'post' dic for\n",
    "                # raw counts (to be used for the information gain function)\n",
    "                if ig_dic:\n",
    "                    temp_dict[cls][key] = val\n",
    "                # Divide the class and attribute pair by the sum of the overall class.\n",
    "                else:\n",
    "                    temp_dict[cls][key] = val/sum_value\n",
    "\n",
    "    return prior, post, training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function should predict the class for an instance or a set of instances, based on a trained model \n",
    "def predict(prior, post, data):\n",
    "    \"\"\"Given instances of classes, predicts the class of each instance using the Naive Bayes' formula.\n",
    "    \n",
    "    Args:\n",
    "        prior (dic): dictionary of prior probabilities of classes.\n",
    "        post (dic): dictionary of posterior probabilities of classes.\n",
    "        data (list): data on which to make predictions w/r/t the classes.\n",
    "    \n",
    "    Returns:\n",
    "        final (dic): dictionary containing the classes of each predicted instance along with their associated probabilities.\n",
    "        data (list): passes back data for use in evaluation metrics.\n",
    "    \"\"\"\n",
    "    # value to be used to ensure probabilities don't hit 'zero', if a given attribute doesn't exist.\n",
    "    EPSILON = 0.000001\n",
    "    final = defaultdict(int)\n",
    "    n_instances = len(data)\n",
    "    # iterate over the data and create nested dictionaries to store predictions.\n",
    "    for i, ins in enumerate(data):\n",
    "        pred = defaultdict(int)\n",
    "        for key, val in prior.items():\n",
    "            # the prediction for the data point needs to be multiplied by the prior and posterior probabilities, \n",
    "            # according to Bayes' formula.\n",
    "            pred[key] = val\n",
    "            for j in range(len(ins)-1):\n",
    "                att = ins[j]\n",
    "                # we will treat '?' as missing data.\n",
    "                if att != '?':\n",
    "                    # if the attribute isn't 'zero' or a '?' then multiply our prediction probability by the next entry.\n",
    "                    if att in post[j][key]:\n",
    "                        pred[key] *= post[j][key][att]\n",
    "                    # otherwise, multiply our prediction probability by an infinitesimally small value.\n",
    "                    else:\n",
    "                        pred[key] *= EPSILON/n_instances\n",
    "        # the predicted class for the instance will be the class that has the largest probability after looping through \n",
    "        # the inner loops.\n",
    "        final[i] = max(pred.items(), key=operator.itemgetter(1))\n",
    "    \n",
    "    return final, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function should evaluate a set of predictions, in a supervised context \n",
    "def evaluate(file, holdout_split=0):\n",
    "    \"\"\"Given a csv file, runs through the pipeline of preprocessing, training, predicting, then evaluating the dataset to \n",
    "    obtain various metrics such as the overall accuracy (disregarding individual class accuracy), and the precision, recall\n",
    "    and F1-score for each class. Also returns a 'ZeroR' Baseline.\n",
    "    \n",
    "    Args:\n",
    "        file (csv file): dataset\n",
    "        holdout_split (number): optional kwarg that splits the data into training and test partitions.\n",
    "    \n",
    "    Returns:\n",
    "        accuracy (number): a numeric value that represents the ratio of the number of instances that matched (true positives)\n",
    "        from the original/test dataset over the number of instances found in the set.\n",
    "        baseline (number): 'zeroR' metric used as a baseline that classes each instant according to the majority class found in\n",
    "        the dataset.\n",
    "        confusion (dictionary of dictionaries): Summarises for each individual class in the dataset the precision, recall and \n",
    "        f1-score (the harmonic mean of precision and recall).\n",
    "    \"\"\"\n",
    "    \n",
    "    # sanity to check to ensure the holdout split is a valid value.\n",
    "    assert (holdout_split >=0 and holdout_split <=1)\n",
    "    if holdout_split > 0:\n",
    "        # get preprocessed data.\n",
    "        cleaned_data_train, cleaned_data_test, n_instances, dic, test_dict, first_split, n_attrs = preprocess(file, split=holdout_split)\n",
    "        # get prior and post probability dictionary objects.\n",
    "        prior,post,_ = train(cleaned_data_train, first_split, dic, n_attrs)\n",
    "        # get the predicted classes, and get back the raw data in a list form for further evaluation.\n",
    "        final, raw = predict(prior, post, cleaned_data_test)\n",
    "\n",
    "    else:   \n",
    "        # same steps as above but for 'holdout' partitioning.\n",
    "        dic,n_instances,cleaned_data,n_attrs = preprocess(file, split=holdout_split)\n",
    "        prior,post,_ = train(cleaned_data, n_instances, dic, n_attrs)\n",
    "        final, raw = predict(prior, post, cleaned_data)\n",
    "    # get a baseline metric based on the majority class for the dataset (zero rule method)\n",
    "    baseline = max(dic.values())/n_instances\n",
    "    hits=0\n",
    "    accuracy=0\n",
    " \n",
    "    recall = defaultdict(int)\n",
    "    prec = defaultdict(int)\n",
    "    # if we have a hit then this is a true positive.\n",
    "    for cls, instance in zip(final.values(), raw):\n",
    "        if cls[0]==instance[-1]:\n",
    "            hits+=1\n",
    "            recall[cls[0]]+=1\n",
    "        else:\n",
    "        #elif (instance[-1]==key):\n",
    "        #    other[key]+=1\n",
    "        # otherwise, we have a FN or FP.\n",
    "            prec[cls[0]]+=1\n",
    "    # determines overall accuracy of the classifier\n",
    "    accuracy = hits/n_instances\n",
    "    final_metrics = defaultdict(int)\n",
    "    # utility function to sort the values in lexicographical order.\n",
    "    sorter_func = lambda x: x[0]\n",
    "    # iterate through our stored classes to obtain metrics for each class in the dataset.\n",
    "    for (a,b), (_, d), (_,f) in zip(sorted(recall.items(), key = sorter_func), sorted(prec.items(), key = sorter_func), \n",
    "                                sorted(dic.items(), key = sorter_func)):\n",
    "        # 'p' holds the precision metric (TP/TP+FP), 'r' holds the recall metric (TP/TP+FN)\n",
    "        p,r = b/(b+d),  b/(b+f)\n",
    "        # 'fs' is the f1-score, the harmonic mean of 'p' and 'r'.\n",
    "        fs = 2*((p*r)/(p+r))\n",
    "        # store the values in order in a dictionary of tuples.\n",
    "        final_metrics[a] = (p, r, fs)\n",
    "    \n",
    "    return accuracy, baseline, final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function should calculate the Information Gain of an attribute or a set of attribute, with respect to the class\n",
    "def info_gain(file):\n",
    "    \"\"\"Determines the information gain attribute for each attribute found in the csv file argument and returns\n",
    "    the attribute found with the highest information gain.\n",
    "    \n",
    "    Args:\n",
    "        file (csv file): dataset\n",
    "    \n",
    "    Returns:\n",
    "        ig (dic): a dictionary containing each attribute in the dataset and their respective information gain, \n",
    "        sorted in descending order.\n",
    "        best_attr(key: value): the attribute that had the maximum information gain the dataset.\n",
    "    \"\"\"\n",
    "    # preprocess the file to get the number of attributes and instances found in the dataset.\n",
    "    dic,n_instances,cleaned_data,n_attrs = preprocess(file)\n",
    "    # obtain a dictionary (ig_dic) of conditional counts of each attribute found in the dataset i.e. given each\n",
    "    # class, how many instances of each attribute's values are found for a given class.\n",
    "    prior,ig_dic,_= train(cleaned_data, n_instances, dic, n_attrs, ig_dic=True)\n",
    "    # determine the class entropy (H(R)) using the raw counts from the prior probs.\n",
    "    class_entropy = -(np.sum([g*np.log2(g) for g in list(prior.values())]))\n",
    "    # Determine the raw counts of each attribute, regardless of their class.\n",
    "    attr_dic= defaultdict(int)\n",
    "    for i in range(n_attrs):\n",
    "        attr_dic[i] = defaultdict(int)\n",
    "        for line in cleaned_data:\n",
    "            attr_dic[i][line[i]]+=1\n",
    "    ar_orig = copy.deepcopy(attr_dic)  \n",
    "    # determine the probability of each attribute's value occurring in the dataset.\n",
    "    probs_mean_info = attr_dic.copy()\n",
    "    for key, ar in attr_dic.items():\n",
    "        probs_mean_info[key] = ar\n",
    "        for key2, val in ar.items():\n",
    "            probs_mean_info[key][key2] = val/n_instances\n",
    "    \n",
    "    \n",
    "    def log_probs(attr_dic, ig_dic, probs_mean_info, attr):\n",
    "        \"\"\"Determines the information gain for each attribute found in the dataset.\n",
    "    \n",
    "        Args:\n",
    "            attr_dic (dic): dictionary containing the raw counts for each attribute found in the dataset.\n",
    "            ig_dic (dic): dictionarying containing the conditional counts for each attribute's values found in the dataset.\n",
    "            probs_mean_info (dic): dictionary containing the (prior) probabilities of finding a given attribute in\n",
    "            the dataset.\n",
    "            attr (number): the attribute which will be evaluated to determine the mean information.\n",
    "            This is an integer corresponding to a key found in the attribute dictionary.\n",
    "    \n",
    "        Returns:\n",
    "            mean_info (numeric value): the 'mean information' for the attribute.\n",
    "        \"\"\"\n",
    "        # sanity check to ensure the user has inputted a valid key.\n",
    "        assert attr in attr_dic.keys()\n",
    "        # dictionary to contain the entropies for the attribute's values. \n",
    "        logs_dic = defaultdict(int)\n",
    "        mean_info = 0\n",
    "        # iterate through the attribute dictionary, only looking at matching keys to our input attribute.\n",
    "        for a, b in attr_dic[attr].items():\n",
    "            for d in ig_dic[attr].values():\n",
    "                for g in d.items():\n",
    "                    # if the keys equal each other, then we multiply the prior prob of the attribute's value\n",
    "                    # by the log of the prior prob of the attribute's value: sum from 1 to the number of types of attr vals\n",
    "                    #where (attr='value') = -P(attr=val|class)*log(P(attr=val|class))\n",
    "                    if a == g[0]:\n",
    "                        logs_dic[a] += -(g[1]/b)*np.log2(g[1]/b)\n",
    "        # the mean information is obtained by multiplying the entropies of the attribute's values by their respective\n",
    "        # prior probabilties of being found in the dataset.\n",
    "        for (a,b),(c,d) in zip(sorted(logs_dic.items()), sorted(probs_mean_info[attr].items())):\n",
    "            mean_info+=b*d\n",
    "        return mean_info\n",
    "    \n",
    "    # iterate through each attribute to find the 'information gain' for each attribute. We subtract the 'mean information'\n",
    "    # for each attribute from H(R).\n",
    "    ig = defaultdict(int)\n",
    "    for i in range(n_attrs):\n",
    "        ig_attr = class_entropy - log_probs(ar_orig, ig_dic, probs_mean_info, i)\n",
    "        ig['attribute: ' + str(i)] = ig_attr\n",
    "    # obtain the attribute which has the greatest information gain.\n",
    "    best_attr = max(ig.items(), key=operator.itemgetter(1))\n",
    "    \n",
    "    # sort the attributes according to the information gain in descending order and return the attribute with the greatest\n",
    "    # information gain.\n",
    "    return sorted(ig.items(), reverse=True, key = lambda x: x[1]), best_attr\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('attribute: 12', 0.0093537102155803464),\n",
       "  ('attribute: 14', 0.005792553705846859),\n",
       "  ('attribute: 15', 0.0057682882016146242),\n",
       "  ('attribute: 16', 0.0057440312456027987),\n",
       "  ('attribute: 13', 0.0040754934196237658),\n",
       "  ('attribute: 17', 0.0025804275555744161),\n",
       "  ('attribute: 5', 0.0013683791752742147),\n",
       "  ('attribute: 2', 0.0012382074503017315),\n",
       "  ('attribute: 4', 0.00099852939063360679),\n",
       "  ('attribute: 1', 0.00091393511608500733),\n",
       "  ('attribute: 9', 0.00089830040440280756),\n",
       "  ('attribute: 6', 0.00054230064444243942),\n",
       "  ('attribute: 8', 0.00048887576912848285),\n",
       "  ('attribute: 0', 0.00044766932839335194),\n",
       "  ('attribute: 7', 0.00043509384646389648),\n",
       "  ('attribute: 3', 0.00014844815831743796),\n",
       "  ('attribute: 11', 7.8684698479491999e-05),\n",
       "  ('attribute: 10', 4.4637788243040433e-05)],\n",
       " ('attribute: 12', 0.0093537102155803464))"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= info_gain('hypothyroid.csv')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(DIR):\n",
    "# 'dir' argument should specify directory containg csv datasets. This function runs all the functions found in the pipeline\n",
    "# to give evaluation metrics.\n",
    "    \n",
    "    count=0\n",
    "    total=0.0\n",
    "    avg = 0.0\n",
    "    total_h=0.0\n",
    "    avg_h=0.0\n",
    "    for file in os.listdir(DIR):\n",
    "        if str(file).endswith('.csv'):\n",
    "            count+=1\n",
    "            acc, base, c = evaluate(file)\n",
    "            acc_h,_, cd = evaluate(file, holdout_split=0.8)\n",
    "            _, best_attr = info_gain(file)\n",
    "            total+=acc\n",
    "            total_h+=acc_h\n",
    "            print(\"TEST ACCURACY for\", str(file).upper(), \"is\", acc, \" \\n and for BASELINE is : \", base, \" \\n and HOLDOUT is: \", acc_h)\n",
    "            print(\"ATTRIBUTE with GREATEST INFORMATION GAIN is: \\n\", best_attr)\n",
    "            print(\"RELATIVE CLASSIFIER performance compared to BASELINE is:\", 100*(acc - base), \"% DIFFERENCE\")\n",
    "            print(\"HOLDOUT performance compared to BASELINE is:\", 100*(acc_h - base), \"% DIFFERENCE\")\n",
    "            print(\"PRECISION, RECALL and F1-score for each class are: \\n \", c.items())\n",
    "            print(\"PRECISION, RECALL and F1-score for HOLDOUT for each class are: \\n\", cd.items())\n",
    "            print()\n",
    "            avg=total/count\n",
    "            avg_h=total_h/count\n",
    "    print(\"MEAN ACCURACY is\", avg, \"and for HOLDOUT it is: \", avg_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST ACCURACY for ANNEAL.CSV is 0.9910913140311804  \n",
      " and for BASELINE is :  0.7616926503340757  \n",
      " and HOLDOUT is:  0.9888888888888889\n",
      "ATTRIBUTE with GREATEST INFORMATION GAIN is: \n",
      " ('attribute: 11', 0.43517783626288553)\n",
      "RELATIVE CLASSIFIER performance compared to BASELINE is: 22.939866369710472 % DIFFERENCE\n",
      "HOLDOUT performance compared to BASELINE is: 22.71962385548132 % DIFFERENCE\n",
      "PRECISION, RECALL and F1-score for each class are: \n",
      "  dict_items([('1', (0.6153846153846154, 0.5, 0.5517241379310345)), ('2', (0.98989898989899, 0.49746192893401014, 0.6621621621621622)), ('3', (0.9970544918998527, 0.49742836149889785, 0.6637254901960784))])\n",
      "PRECISION, RECALL and F1-score for HOLDOUT for each class are: \n",
      " dict_items([('1', (0.3333333333333333, 0.125, 0.18181818181818182))])\n",
      "\n",
      "TEST ACCURACY for BREAST-CANCER.CSV is 0.7552447552447552  \n",
      " and for BASELINE is :  0.7027972027972028  \n",
      " and HOLDOUT is:  0.7931034482758621\n",
      "ATTRIBUTE with GREATEST INFORMATION GAIN is: \n",
      " ('attribute: 5', 0.077009852516614408)\n",
      "RELATIVE CLASSIFIER performance compared to BASELINE is: 5.24475524475524 % DIFFERENCE\n",
      "HOLDOUT performance compared to BASELINE is: 9.030624547865928 % DIFFERENCE\n",
      "PRECISION, RECALL and F1-score for each class are: \n",
      "  dict_items([('no-recurrence-events', (0.8133971291866029, 0.4582210242587601, 0.5862068965517241)), ('recurrence-events', (0.5974025974025974, 0.3511450381679389, 0.4423076923076923))])\n",
      "PRECISION, RECALL and F1-score for HOLDOUT for each class are: \n",
      " dict_items([('no-recurrence-events', (0.82, 0.20707070707070707, 0.33064516129032256)), ('recurrence-events', (0.625, 0.06578947368421052, 0.11904761904761904))])\n",
      "\n",
      "TEST ACCURACY for CAR.CSV is 0.8738425925925926  \n",
      " and for BASELINE is :  0.7002314814814815  \n",
      " and HOLDOUT is:  0.8728323699421965\n",
      "ATTRIBUTE with GREATEST INFORMATION GAIN is: \n",
      " ('attribute: 5', 0.26218435655426375)\n",
      "RELATIVE CLASSIFIER performance compared to BASELINE is: 17.361111111111104 % DIFFERENCE\n",
      "HOLDOUT performance compared to BASELINE is: 17.260088846071497 % DIFFERENCE\n",
      "PRECISION, RECALL and F1-score for each class are: \n",
      "  dict_items([('vgood', (0.9512195121951219, 0.375, 0.5379310344827586)), ('good', (0.6363636363636364, 0.23333333333333334, 0.34146341463414637)), ('unacc', (0.9317817014446228, 0.48966680725432304, 0.6419684821675421)), ('acc', (0.7083333333333334, 0.4294205052005943, 0.5346901017576318))])\n",
      "PRECISION, RECALL and F1-score for HOLDOUT for each class are: \n",
      " dict_items([('vgood', (0.8571428571428571, 0.1016949152542373, 0.18181818181818182)), ('good', (0.45454545454545453, 0.078125, 0.1333333333333333)), ('unacc', (0.94140625, 0.20150501672240803, 0.3319559228650138)), ('acc', (0.6944444444444444, 0.136986301369863, 0.22883295194508008))])\n",
      "\n",
      "TEST ACCURACY for CMC.CSV is 0.5057705363204344  \n",
      " and for BASELINE is :  0.4270196877121521  \n",
      " and HOLDOUT is:  0.45084745762711864\n",
      "ATTRIBUTE with GREATEST INFORMATION GAIN is: \n",
      " ('attribute: 2', 0.10173991727554088)\n",
      "RELATIVE CLASSIFIER performance compared to BASELINE is: 7.8750848608282356 % DIFFERENCE\n",
      "HOLDOUT performance compared to BASELINE is: 2.3827769914966543 % DIFFERENCE\n",
      "PRECISION, RECALL and F1-score for each class are: \n",
      "  dict_items([('Long-term', (0.39544513457556935, 0.36450381679389315, 0.37934458788480635)), ('Short-term', (0.4738219895287958, 0.2615606936416185, 0.33705772811918067)), ('No-use', (0.6134868421052632, 0.3722554890219561, 0.46335403726708074))])\n",
      "PRECISION, RECALL and F1-score for HOLDOUT for each class are: \n",
      " dict_items([('Long-term', (0.3469387755102041, 0.11074918566775244, 0.1679012345679012)), ('Short-term', (0.37209302325581395, 0.07174887892376682, 0.12030075187969926)), ('No-use', (0.6036036036036037, 0.12007168458781362, 0.20029895366218237))])\n",
      "\n",
      "TEST ACCURACY for HEPATITIS.CSV is 0.8516129032258064  \n",
      " and for BASELINE is :  0.7935483870967742  \n",
      " and HOLDOUT is:  0.9354838709677419\n",
      "ATTRIBUTE with GREATEST INFORMATION GAIN is: \n",
      " ('attribute: 10', 0.12834719717550025)\n",
      "RELATIVE CLASSIFIER performance compared to BASELINE is: 5.806451612903219 % DIFFERENCE\n",
      "HOLDOUT performance compared to BASELINE is: 14.193548387096765 % DIFFERENCE\n",
      "PRECISION, RECALL and F1-score for each class are: \n",
      "  dict_items([('LIVE', (0.923728813559322, 0.4698275862068966, 0.6228571428571429)), ('DIE', (0.6216216216216216, 0.41818181818181815, 0.5))])\n",
      "PRECISION, RECALL and F1-score for HOLDOUT for each class are: \n",
      " dict_items([('LIVE', (0.9629629629629629, 0.21311475409836064, 0.348993288590604)), ('DIE', (0.75, 0.0967741935483871, 0.1714285714285714))])\n",
      "\n",
      "TEST ACCURACY for HYPOTHYROID.CSV is 0.9522605121719886  \n",
      " and for BASELINE is :  0.9522605121719886  \n",
      " and HOLDOUT is:  0.9510268562401264\n",
      "ATTRIBUTE with GREATEST INFORMATION GAIN is: \n",
      " ('attribute: 12', 0.0093537102155803464)\n",
      "RELATIVE CLASSIFIER performance compared to BASELINE is: 0.0 % DIFFERENCE\n",
      "HOLDOUT performance compared to BASELINE is: -0.12336559318621987 % DIFFERENCE\n",
      "PRECISION, RECALL and F1-score for each class are: \n",
      "  dict_items([('negative', (0.9522605121719886, 0.9522605121719886, 0.9522605121719886))])\n",
      "PRECISION, RECALL and F1-score for HOLDOUT for each class are: \n",
      " dict_items([('negative', (0.9510268562401264, 0.8337950138504155, 0.8885608856088562))])\n",
      "\n",
      "TEST ACCURACY for MUSHROOM.CSV is 0.9971688823239784  \n",
      " and for BASELINE is :  0.517971442639094  \n",
      " and HOLDOUT is:  0.9969230769230769\n",
      "ATTRIBUTE with GREATEST INFORMATION GAIN is: \n",
      " ('attribute: 4', 0.90607497738399978)\n",
      "RELATIVE CLASSIFIER performance compared to BASELINE is: 47.91974396848844 % DIFFERENCE\n",
      "HOLDOUT performance compared to BASELINE is: 47.89516342839829 % DIFFERENCE\n",
      "PRECISION, RECALL and F1-score for each class are: \n",
      "  dict_items([('e', (0.9992841803865425, 0.49880895664602193, 0.6654484785890203)), ('p', (0.9949148232901094, 0.49980840464938053, 0.6653630334977045))])\n",
      "PRECISION, RECALL and F1-score for HOLDOUT for each class are: \n",
      " dict_items([('e', (0.998812351543943, 0.2000475737392959, 0.3333333333333333)), ('p', (0.9948914431673053, 0.19897828863346104, 0.3316304810557684))])\n",
      "\n",
      "TEST ACCURACY for NURSERY.CSV is 0.9030864197530865  \n",
      " and for BASELINE is :  0.3333333333333333  \n",
      " and HOLDOUT is:  0.9012345679012346\n",
      "ATTRIBUTE with GREATEST INFORMATION GAIN is: \n",
      " ('attribute: 7', 0.95877496046997623)\n",
      "RELATIVE CLASSIFIER performance compared to BASELINE is: 56.97530864197531 % DIFFERENCE\n",
      "HOLDOUT performance compared to BASELINE is: 56.790123456790134 % DIFFERENCE\n",
      "PRECISION, RECALL and F1-score for each class are: \n",
      "  dict_items([('not_recom', (0.8372093023255814, 0.5, 0.6260869565217391)), ('priority', (0.9029535864978903, 0.4745011086474501, 0.622093023255814)), ('spec_prior', (0.9994308480364257, 0.9994308480364257, 0.9994308480364257))])\n",
      "PRECISION, RECALL and F1-score for HOLDOUT for each class are: \n",
      " dict_items([('not_recom', (0.8326885880077369, 0.19930555555555557, 0.3216286888307807)), ('priority', (0.9042675893886967, 0.1874252928520201, 0.3104950495049505))])\n",
      "\n",
      "TEST ACCURACY for PRIMARY-TUMOR.CSV is 0.6047197640117994  \n",
      " and for BASELINE is :  0.24778761061946902  \n",
      " and HOLDOUT is:  0.4852941176470588\n",
      "ATTRIBUTE with GREATEST INFORMATION GAIN is: \n",
      " ('attribute: 2', 0.55235262338090285)\n",
      "RELATIVE CLASSIFIER performance compared to BASELINE is: 35.693215339233035 % DIFFERENCE\n",
      "HOLDOUT performance compared to BASELINE is: 23.75065070275898 % DIFFERENCE\n",
      "PRECISION, RECALL and F1-score for each class are: \n",
      "  dict_items([('F', (0.25, 0.5, 0.3333333333333333)), ('H', (0.1, 0.25, 0.14285714285714288)), ('M', (0.6666666666666666, 0.2222222222222222, 0.3333333333333333)), ('B', (0.8333333333333334, 0.5, 0.625)), ('J', (0.2, 0.5, 0.28571428571428575)), ('A', (0.8048780487804879, 0.44, 0.5689655172413793)), ('C', (0.2222222222222222, 0.18181818181818182, 0.19999999999999998)), ('D', (0.2, 0.2631578947368421, 0.22727272727272727)), ('G', (0.041666666666666664, 0.06666666666666667, 0.05128205128205127)), ('K', (0.5555555555555556, 0.3488372093023256, 0.4285714285714286)), ('L', (0.42857142857142855, 0.42857142857142855, 0.42857142857142855)), ('E', (0.8333333333333334, 0.20408163265306123, 0.3278688524590165)), ('N', (0.6923076923076923, 0.2727272727272727, 0.391304347826087))])\n",
      "PRECISION, RECALL and F1-score for HOLDOUT for each class are: \n",
      " dict_items([('A', (0.7222222222222222, 0.16883116883116883, 0.2736842105263158)), ('Q', (0.2222222222222222, 0.15384615384615385, 0.18181818181818185)), ('H', (0.1111111111111111, 0.09090909090909091, 0.09999999999999999)), ('B', (0.8333333333333334, 0.2631578947368421, 0.39999999999999997)), ('K', (0.3333333333333333, 0.03125, 0.05714285714285714)), ('R', (0.7142857142857143, 0.5555555555555556, 0.6250000000000001)), ('V', (0.5, 0.5, 0.5)), ('E', (0.75, 0.2727272727272727, 0.39999999999999997)), ('N', (0.3333333333333333, 0.5, 0.4))])\n",
      "\n",
      "MEAN ACCURACY is 0.8260886310750691 and for HOLDOUT it is:  0.8195149616014783\n"
     ]
    }
   ],
   "source": [
    "main('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The Naive Bayes classifiers can be seen to vary, in terms of their effectiveness on the given datasets (e.g. in terms of Accuracy). Consider the Information Gain of each attribute, relative to the class distribution — does this help to explain the classifiers’ behaviour? Identify any results that are particularly surprising, and explain why they occur.\n",
    "\n",
    "\n",
    "An interesting relationship can be seen between the attributes with the maximal information gain, and the overall accuracy metric, when there was a disparity between the accuracy and the ZeroR benchmark. For example, in 'hypothroid.csv' 95% of its instances were of class 'negative'. The classifier performed the same as the ZeroR benchmark in this instance and was the only example of where the classifier perfomed the same as the benchmark.\n",
    "The attribute with the largest information gain for that dataset had an information gain of 0.009, indicating that there was little information to be gained from individual attributes that would help discern the classifier between the two classes in this dataset. This may also indicate that their wasn't much correlation between the various attributes and the two classes and shows how misleading the accuracy benchmark can be, as the classifier obtained a 95% class accuracy. This could be due to a large class imbalance between the large number of negative instances and small number of 'hypothroid' instances, and if the attributes don't correlate much to the predictions, then the classifier is probably using the large prior probability of the 'negative' instances to make predictions, and using little of the posterior probabilities extracted from Bayes' formula in making predictions.\n",
    "Likewise, for the dataset 'breast_cancer.csv', there was little difference\n",
    "between the benchmark and the classifier's performance, and the information gain for each attribute was extremely low.\n",
    "Conversely, when the classifier performed much better than the benchmark, such as in 'nursery.csv', the attribute with the largest information gain (attribute 7, corresponding to 'health) had quite a large value (0.96 information gain).\n",
    "This is true of other datasets which performed much better than the benchmark such as the 'mushroom.csv' dataset. \n",
    "This shows a strong positive correlation between a well-performing classifier and an attribute/s that have a large information gain.\n",
    "It seems that the classifier was performing much better than the baseline benchmark when there were attributes that were strongly correlated with particular classes, which is indicated by the relatively linear relationship between large information gain attributes and effective classifiers. This suggests that for those kinds of datasets, some of the attributes were effective\n",
    "in contributing to a discriminative effect between the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Evaluating the model on the same data that we use to train the model is considered to be a major mistake in Machine Learning. Implement a hold–out or cross–validation evaluation strategy. How does your estimate of effectiveness change, compared to testing on the training data? Explain why. (The result might surprise you!)\n",
    "\n",
    "Using a holdout strategy of an 80% training, 20% test split, the overall accuracy was surprisingly similar to results when training on the full model (c.f. above cells where MEAN ACCURACY is 0.8260886310750691 and for HOLDOUT it is:  0.8195149616014783).\n",
    "Even with varying the holdout partitions to smaller training partitions, there wasn't a sharp difference in the accuracy.\n",
    "For example, with using a training partition of only 30% of the data, the overall accuracy was only approximately 4% lower than training on the full dataset.\n",
    "Addditionally, there were individual datasets where the holdout classifier outperformed the classifier trained on the full dataset (hepatitis.csv, breast-cancer.csv).\n",
    "These results are quite surprising and counter-intuitive, owing to the inherent bias with training on a full dataset. One would also assume that training on a partition that consisted of a small fraction of the data could mean the classifier wouldn't have enough data to infer solid enough relationships between attributes and different classes, or that there may be class imbalances present that would mean the classifier wouldn't be able to infer strong correlations between individual attributes and the classes. \n",
    "The fact that the holdout strategy, even when used on small training partitions, performed quite similarly to the full classifier may suggest that there were fairly uniform \"attribute=val|class\" distributions in *most* of the datasets, such that the posterior probabilities were consistent amongst random partitions. For example, P(attr(x)=1|class(A))=0.95, P(attr(x)=2|class(B))=0.9, so that even with a relatively smaller training partition, testing accuracy is similar to training on the full dataset/s as there were attributes in those datasets that were effective at discriminating between classes (evidenced by large information gain).\n",
    "Even with datasets where there was evidently nittle discriminatory information found in all the attributes (such as hypothroid.csv), the holdout strategy performed similarly to the full classifier as well as the ZeroR benchmark, probably because heavy prediction weights were attributed to the prior probabilities of the classes and there were huge class imbalances in those datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a,b = info_gain('primary-tumor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('attribute: 2', 0.55235262338090285),\n",
       " ('attribute: 3', 0.37910426027718858),\n",
       " ('attribute: 1', 0.3353600515055537),\n",
       " ('attribute: 12', 0.29153013602249223),\n",
       " ('attribute: 14', 0.24588868143377063),\n",
       " ('attribute: 8', 0.22052193470670511),\n",
       " ('attribute: 4', 0.21246189904816459),\n",
       " ('attribute: 9', 0.19976143639025112),\n",
       " ('attribute: 15', 0.18425767171538388),\n",
       " ('attribute: 16', 0.1701481108388716),\n",
       " ('attribute: 0', 0.15474214188705782),\n",
       " ('attribute: 13', 0.12715354518198163),\n",
       " ('attribute: 6', 0.10088123982398978),\n",
       " ('attribute: 7', 0.067872775704422406),\n",
       " ('attribute: 10', 0.067144602410105225),\n",
       " ('attribute: 11', 0.06025390884525228),\n",
       " ('attribute: 5', 0.020366938848049188)]"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
